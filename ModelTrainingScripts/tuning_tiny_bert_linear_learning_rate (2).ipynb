{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxHLxouEHvnJ"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "jp797498e_twitter_entity_sentiment_analysis_path = kagglehub.dataset_download('jp797498e/twitter-entity-sentiment-analysis')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU4JnlOAHvni",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf34G1ENHvnl",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-29T16:33:37.241106Z",
          "iopub.status.busy": "2025-08-29T16:33:37.24085Z"
        },
        "id": "0cLUZrPlHvnm",
        "outputId": "94ced786-1be6-4119-f68c-a69a1f2db06e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets accelerate -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlYK8p-2Hvnu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import warnings\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available(): print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
        "os.makedirs(\"./sentiment_model\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOg7IfLcHvn0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "class Config:\n",
        "    MODEL_NAME = 'prajjwal1/bert-tiny'\n",
        "    DATA_PATH = '/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv'\n",
        "    SAVE_PATH = './sentiment_model'\n",
        "    MAX_LEN = 128\n",
        "    EPOCHS = 30\n",
        "    BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "\n",
        "    LEARNING_RATE = 2e-5\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "config = Config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzgDiCj6Hvn5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\n--- Loading and Preparing Data ---\")\n",
        "df = pd.read_csv(config.DATA_PATH, header=None)\n",
        "df.columns = ['id', 'entity', 'label', 'text']\n",
        "\n",
        "\n",
        "df = df[df['label'] != 'Irrelevant']\n",
        "label_map = {'Positive': 2, 'Neutral': 1, 'Negative': 0}\n",
        "\n",
        "df['label'] = df['label'].map(label_map)\n",
        "df.dropna(subset=['text'], inplace=True)\n",
        "df = df.reset_index(drop=True)\n",
        "train_df, val_df = train_test_split(df, test_size=0.15, random_state=42, stratify=df['label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOZPI9A1Hvn8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
        "class SentimentClassificationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer, self.text, self.labels, self.max_len = tokenizer, dataframe.text.tolist(), dataframe.label.tolist(), max_len\n",
        "    def __len__(self): return len(self.text)\n",
        "    def __getitem__(self, index):\n",
        "        text, label = str(self.text[index]), self.labels[index]\n",
        "        encoding = self.tokenizer.encode_plus(text, add_special_tokens=True, max_length=self.max_len, return_token_type_ids=False, padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt')\n",
        "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'labels': torch.tensor(label, dtype=torch.long)}\n",
        "\n",
        "train_dataset = SentimentClassificationDataset(train_df, tokenizer, config.MAX_LEN)\n",
        "val_dataset = SentimentClassificationDataset(val_df, tokenizer, config.MAX_LEN)\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7fPQknjHvn_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(config.MODEL_NAME, num_labels=3)\n",
        "if torch.cuda.device_count() > 1: model = torch.nn.DataParallel(model)\n",
        "model.to(config.DEVICE)\n",
        "optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
        "loss_fn = torch.nn.CrossEntropyLoss().to(config.DEVICE)\n",
        "\n",
        "total_training_steps = len(train_loader) * config.EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0, \n",
        "    num_training_steps=total_training_steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvBl02YqHvoD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(data_loader, desc=\"Training\", leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        if isinstance(model, torch.nn.DataParallel): loss = loss.mean()\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
        "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            if isinstance(model, torch.nn.DataParallel): loss = loss.mean()\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    report_str = classification_report(all_labels, all_preds, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0)\n",
        "    report_dict = classification_report(all_labels, all_preds, zero_division=0, output_dict=True)\n",
        "    weighted_f1 = report_dict['weighted avg']['f1-score']\n",
        "    return total_loss / len(data_loader), accuracy, report_str, weighted_f1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhMd5g1sHvoH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "history = {'train_loss': [], 'val_loss': [], 'val_accuracy': [], 'val_f1': []}\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(config.EPOCHS):\n",
        "    print(f'\\n--- Epoch {epoch + 1}/{config.EPOCHS} ---')\n",
        "    train_loss = train_epoch(model, train_loader, loss_fn, optimizer, config.DEVICE, scheduler)\n",
        "    print(f'Training Loss: {train_loss:.4f}')\n",
        "\n",
        "    val_loss, val_acc, val_report_str, val_weighted_f1 = eval_model(model, val_loader, loss_fn, config.DEVICE)\n",
        "    print(f'Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc:.4f}')\n",
        "    print(\"Classification Report:\\n\", val_report_str)\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_accuracy'].append(val_acc)\n",
        "    history['val_f1'].append(val_weighted_f1)\n",
        "\n",
        "    if val_acc > best_accuracy:\n",
        "        best_accuracy = val_acc\n",
        "        model_to_save = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
        "        model_to_save.save_pretrained(config.SAVE_PATH)\n",
        "        tokenizer.save_pretrained(config.SAVE_PATH)\n",
        "        print(f\"\\nNew best model saved to {config.SAVE_PATH} with accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(f\"Best validation accuracy achieved: {best_accuracy:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfhC5SUgHvoK",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cCwf3OyHvoO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\n--- Plotting Training History ---\")\n",
        "plt.style.use('ggplot')\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "epochs_range = range(1, config.EPOCHS + 1)\n",
        "ax1.plot(epochs_range, history['train_loss'], 'b-o', label='Training Loss')\n",
        "ax1.plot(epochs_range, history['val_loss'], 'r-o', label='Validation Loss')\n",
        "ax1.set_title('Training and Validation Loss'); ax1.set_xlabel('Epochs'); ax1.set_ylabel('Loss'); ax1.legend(); ax1.grid(True)\n",
        "ax2.plot(epochs_range, history['val_accuracy'], 'g-o', label='Validation Accuracy')\n",
        "ax2.plot(epochs_range, history['val_f1'], 'm-o', label='Validation F1-Score')\n",
        "ax2.set_title('Validation Accuracy and F1-Score'); ax2.set_xlabel('Epochs'); ax2.set_ylabel('Score'); ax2.legend(); ax2.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvtxwGLDHvoP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\n--- Loading saved model for inference test ---\")\n",
        "final_model = AutoModelForSequenceClassification.from_pretrained(config.SAVE_PATH)\n",
        "final_tokenizer = AutoTokenizer.from_pretrained(config.SAVE_PATH)\n",
        "\n",
        "final_model.to(config.DEVICE)\n",
        "final_model.eval()\n",
        "def predict_sentiment(text):\n",
        "    inputs = final_tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=config.MAX_LEN,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "    input_ids = inputs['input_ids'].to(config.DEVICE)\n",
        "    attention_mask = inputs['attention_mask'].to(config.DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = final_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "    confidence = torch.nn.functional.softmax(outputs.logits, dim=1).max().item()\n",
        "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "    return reverse_label_map[prediction], confidence\n",
        "\n",
        "test_texts = [\n",
        "    \"This game is absolutely fantastic, I can't stop playing!\",\n",
        "    \"The customer service was terrible and I had to wait for hours.\",\n",
        "    \"It's an okay product, not great but not bad either.\",\n",
        "]\n",
        "\n",
        "print(\"\\n--- Inference Examples ---\")\n",
        "for text in test_texts:\n",
        "    sentiment, conf = predict_sentiment(text)\n",
        "    print(f\"Text: '{text}'\\nPredicted Sentiment: {sentiment} (Confidence: {conf:.4f})\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx06RpEsHvoU",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toqmvF5dHvoU",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6imPu_vHvoU",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjkRiI1fHvoV",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chySYtG2HvoV",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy62NWhNHvoW",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZL2DEMPHvoZ",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4CR8zGLHvoZ",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyXoBmt3Hvoa",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4AZR5cxHvob",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE56uiWRHvob",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "tuning-tiny-bert_linear_learning_rate",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 1520310,
          "sourceId": 2510329,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31090,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
